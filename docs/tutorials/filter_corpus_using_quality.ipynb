{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering corpora using Quality\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/HLasse/TextDescriptives/blob/main/docs/tutorials/filter_corpus_using_quality.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In many cases if you want to analyse tweets, train a model on text scraped from the web or similar, it is important to filter out low-quality texts.\n",
    "\n",
    "TextDescriptives implements a series of heuristic filters for removing low-quality text. This tutorial will take you through how to use these to filter\n",
    "your text corpora."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "For this we will use datasets available on [Huggingface Datasets](https://huggingface.co/datasets). Thus we will need the `datasets` package. Which you can install by running\n",
    "\n",
    "```python\n",
    "!pip install datasets\n",
    "```\n",
    "\n",
    "Or by installing textdescriptives with the `[tutorials]` option as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import textdescriptives as td\n",
    "except:\n",
    "    !pip install \"textdescriptives[tutorials]\"\n",
    "    import textdescriptives as td"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Web content\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The Data\n",
    "For our first example we will filter web content. For this we will use the [mC4 dataset](https://huggingface.co/datasets/mc4). It would take ages to download the whole data so instead we will stream down 1000 samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# stream in the dataset\n",
    "dataset = load_dataset(\n",
    "    \"mc4\", \"en\", streaming=True, split=\"train\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# download the first 1 000\n",
    "dataset = dataset.take(1000)\n",
    "\n",
    "# extract the text\n",
    "texts = [sample[\"text\"] for sample in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts 4,362\tMore Info\n",
      "Okay so to those of you that were very helpful this is not to you but for those of you that laugh when I ask about ohms or powering LSi15's this is to you. If you know a book, website, or someone to talk to to get more info that I seek so I know what some of you are talking about please share it with me. I ask questions to gain more info on audio thats all. Not to get laughed\n"
     ]
    }
   ],
   "source": [
    "# let us look at the first part (400 characters) of the first text\n",
    "print(texts[0][:400])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "To filter texts using `textdescriptives` we need to first set up the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# create the spacy nlp pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "# add a component for sentence segmentation\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "# add a component for quality filtering\n",
    "quality_pipe = nlp.add_pipe(\"textdescriptives/quality\")\n",
    "\n",
    "# apply the pipeline to the texts\n",
    "docs = nlp.pipe(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will note here that docs is a generator. This can be quite useful (especially when streaming texts in one at a time), but for this example we can simply convert it to a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs is type <class 'generator'>\n",
      "docs is type <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"docs is type {type(docs)}\")\n",
    "docs = list(docs)\n",
    "print(f\"docs is type {type(docs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is easy to examine the documents using the `doc._.quality` or `doc._.passed_quality_check` extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts 4,362\tMore Info\n",
      "Okay so to those of you that were very helpful this is not to you but for those of you that laugh when I ask about ohms or powering LSi15's this is to you. If you know a book, website, or someone to talk to to get more info that I seek so I know what some of you are talking about please share it with me. I ask questions to gain more info on audio thats all. Not to get laughed at when asking it.\n"
     ]
    }
   ],
   "source": [
    "# examine the first document\n",
    "doc = docs[0]\n",
    "print(doc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.passed_quality_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this document did no pass the quality check. Let us examine why that is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QualityOutput(\n",
       "\tpassed=False, \n",
       "\tn_stop_words=ThresholdsOutput(value=435.0, passed=True, threshold=(2.0, None)), \n",
       "\talpha_ratio=ThresholdsOutput(value=0.79, passed=True, threshold=(0.7, None)), \n",
       "\tmean_word_length=ThresholdsOutput(value=3.52, passed=True, threshold=(3.0, 10.0)), \n",
       "\tdoc_length=ThresholdsOutput(value=894.0, passed=True, threshold=(10.0, 100000.0)), \n",
       "\tsymbol_to_word_ratio={'#': ThresholdsOutput(value=0.0, passed=True, threshold=(None, 0.1))}, \n",
       "\tproportion_ellipsis=ThresholdsOutput(value=0.0, passed=True, threshold=(None, 0.3)), \n",
       "\tproportion_bullet_points=ThresholdsOutput(value=0.0, passed=True, threshold=(None, 0.8)), \n",
       "\tcontains={'lorem ipsum': ThresholdsOutput(value=0.0, passed=True, threshold=False)}, \n",
       "\tduplicate_line_chr_fraction=ThresholdsOutput(value=0.0, passed=True, threshold=(None, 0.2)), \n",
       "\tduplicate_paragraph_chr_fraction=ThresholdsOutput(value=0.0, passed=True, threshold=(None, 0.2)), \n",
       "\tduplicate_ngram_chr_fraction={'5': ThresholdsOutput(value=0.42, passed=False, threshold=(None, 0.15)), '6': ThresholdsOutput(value=0.42, passed=False, threshold=(None, 0.14)), '7': ThresholdsOutput(value=0.38, passed=False, threshold=(None, 0.13)), '8': ThresholdsOutput(value=0.36, passed=False, threshold=(None, 0.12)), '9': ThresholdsOutput(value=0.36, passed=False, threshold=(None, 0.11)), '10': ThresholdsOutput(value=0.36, passed=False, threshold=(None, 0.1))}, \n",
       "\ttop_ngram_chr_fraction={'2': ThresholdsOutput(value=0.01, passed=True, threshold=(None, 0.2)), '3': ThresholdsOutput(value=0.01, passed=True, threshold=(None, 0.18)), '4': ThresholdsOutput(value=0.01, passed=True, threshold=(None, 0.16))})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc._.quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, you might not know what all of these mean, but you can easily check it on [the documentation site](https://hlasse.github.io/TextDescriptives/quality.html). Examining these we see that this text has a high proportion of characters which appear in duplicate n-grams `duplicate_10-gram_chr_fraction`. When this fraction is really high it means that the text contains a high proportion of repititions. This is often a sign of low quality text.\n",
    "\n",
    "If we examine the quality thresholds of the pipeline we can see that the max allowed value for `duplicate_10-gram_chr_fraction` is 0.1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_stop_words=(2, None) alpha_ratio=(0.7, None) mean_word_length=(3, 10) doc_length=(10, 100000) symbol_to_word_ratio={'#': (None, 0.1)} proportion_ellipsis=(None, 0.3) proportion_bullet_points=(None, 0.8) contains={'lorem ipsum': False} duplicate_line_chr_fraction=(None, 0.2) duplicate_paragraph_chr_fraction=(None, 0.2) duplicate_ngram_chr_fraction={'5': (None, 0.15), '6': (None, 0.14), '7': (None, 0.13), '8': (None, 0.12), '9': (None, 0.11), '10': (None, 0.1)} top_ngram_chr_fraction={'2': (None, 0.2), '3': (None, 0.18), '4': (None, 0.16)}\n",
      "---\n",
      "The thresholds for Duplicate n-grams:\n",
      "{'5': (None, 0.15), '6': (None, 0.14), '7': (None, 0.13), '8': (None, 0.12), '9': (None, 0.11), '10': (None, 0.1)}\n"
     ]
    }
   ],
   "source": [
    "print(quality_pipe.quality_thresholds)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"The thresholds for Duplicate n-grams:\")\n",
    "print(quality_pipe.quality_thresholds.duplicate_ngram_chr_fraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting high quality texts\n",
    "We are typically interested in text which are not of low quality. We can extract these by filtering out the texts which did not pass the quality check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_texts = [doc for doc in docs if doc._.passed_quality_check]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 1000 texts were processed and 572 passed the quality check.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"A total of {len(docs)} texts were processed and {len(filtered_texts)} passed the quality check.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the filters\n",
    "In some cases you might want to apply other filters. For instance the current filter sets a `symbol_to_word_ratio` threshold of 0.1 for hashtags `#`. This means that if a text contains a lot of hashtags it will be filtered out. However if you are working on e.g. tweets this is an unreasonable filter and you might want to adjust that. You can do this by overwriting the quality_thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_thresholds = td.QualityThresholds(\n",
    "    n_stop_words=(2, None),  # at least 2 stop words, no upper bound\n",
    "    alpha_ratio=(0.7, None),\n",
    "    mean_word_length=(3, 10),  # mean word length between 3 and 10 characters\n",
    "    doc_length=(10, 100_000),\n",
    "    symbol_to_word_ratio={},  # don't filter based on symbol to word ratio.\n",
    "    proportion_ellipsis=(None, 0.3),\n",
    "    proportion_bullet_points=(None, 0.8),\n",
    "    contains={\n",
    "        \"lorem ipsum\": False\n",
    "    },  # remove texts which contain the string \"lorem ipsum\"\n",
    "    duplicate_line_chr_fraction=(None, 0.2),\n",
    "    duplicate_paragraph_chr_fraction=(None, 0.2),\n",
    "    duplicate_ngram_chr_fraction={},  # don't filter based on duplicate n-grams\n",
    "    top_ngram_chr_fraction={\"2\": (None, 0.2), \"3\": (None, 0.18), \"4\": (None, 0.16)},\n",
    ")\n",
    "\n",
    "# overwrite the existing thresholds\n",
    "quality_pipe.set_quality_thresholds(new_thresholds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read more about what each argument does, please check out the [documentation](https://hlasse.github.io/TextDescriptives/quality.html#data-classes).\n",
    "All the `passed` values and `passed_quality_check` attributes are dynamically updated when you can `.set_quality_thresholds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the new text now pass the quality filter\n",
    "doc._.passed_quality_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Domains\n",
    "\n",
    "These quality metrics are heuristic based and need to be tuned. While the defaults are reasonable for some domains, they may not be for others. We will explore this a bit further in this section. These filters are specifically tuned for the web domain and this can lead to problems when applied directly to other domains.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "For this we will use the [Danish Gigaword](https://sprogteknologi.dk/dataset/danish-gigaword) available on [Huggingface Datasets](DDSC/partial-danish-gigaword-no-twitter). For the purpose of this tutorial we will just use a small test version of it containing around 2500 examples, but you could easily change it to use the whole dataset. Danish Gigaword is a large collection of Danish texts collected from a variety of domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can download the dataset using the following command:\n",
    "\n",
    "#### NOTE: The DAGW dataset is currently unavailable due to a copyright dispute. The remainder of the tutorial has been disabled for now (converted to markdown), and will be re-enabled once the dispute settles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# note that this can take a little while\n",
    "dataset = load_dataset(\"DDSC/partial-danish-gigaword-small-test-sample\")\n",
    "\n",
    "# All of the dataset is available in the train split\n",
    "dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# We can take a look at one of the examples:\n",
    "ten_samples = dataset.select(range(10))\n",
    "ten_samples.to_pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, the Danish Gigaword corpus consist of multiple domains. For this tutorial, we will look at three of these domains. `retsinformationdk` which consists of legal documents, `hest` which contains post from a Danish debate forum ([heste-nettet.dk](https://www.heste-nettet.dk/)) and `spont` which contains texts transcribed from spontaneous speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# we can filter out these three datasets based on the \"source\"\n",
    "legal = dataset.filter(lambda x: x[\"source\"] == \"retsinformationdk\", num_proc=1)\n",
    "news = dataset.filter(lambda x: x[\"source\"] == \"tv2r\", num_proc=1)\n",
    "speech = dataset.filter(lambda x: x[\"source\"] == \"spont\", num_proc=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine these datasets a bit more:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print(f\"Legal contains {len(legal)} examples\")\n",
    "print(f\"News contains {len(news)} examples\")\n",
    "print(f\"Speech contains {len(speech)} examples\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can for example see that the speech dataset contains notably fewer samples than the others. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Filtering\n",
    "After we have prepared our datasets we can now start with the quality filtering. Using TextDescriptives, this is extremely simple. We need to do 3 things:\n",
    "\n",
    "1) Create a pipeline\n",
    "2) Add the quality component to it\n",
    "3) Apply the pipeline to the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# 1. Crease a blank spaCy model with a sentencizer as that's the only component required for the quality metrics\n",
    "nlp = spacy.blank(\"da\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "nlp.max_length = (\n",
    "    2000000  # as some of the documents are quite long we can increase the max length\n",
    ")\n",
    "# however it might be worth filtering out these documents beforehand for very very long documents.\n",
    "\n",
    "# 2. Add the textdescriptives pipeline\n",
    "quality_pipe = nlp.add_pipe(\"textdescriptives/quality\")\n",
    "\n",
    "# 3. Apply the pipeline to the legal documents\n",
    "legal_docs = nlp.pipe(legal[\"text\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check now we can see that legal_docs is a generator. This can be a quite efficient format, but for now we just want to process all the text so we simply need to convert it to a list:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "legal_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "legal_docs = list(legal_docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the output here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "legal_doc = legal_docs[0]\n",
    "\n",
    "print(legal_doc[:100])  # print the first 100 tokens\n",
    "print(\"----\")\n",
    "print(\"This passed the quality filter:\")\n",
    "legal_doc._.passed_quality_check"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the text did not pass the quality filter. We can now examine why that using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "legal_doc._.quality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that fraction of characters which is a part of a duplicate 10 gram is >50%. This is a reason why the sample was filtered out. This is not uncommon for legal documents which contain a lot of standard phrases. However you might wish to change the threshold for this filter. We showed you have to do this in the previous section. We also see that the `alpha_ratio` is close 0.8. This means that the text is mostly made up of alphabetic characters. This is good, but as we will see later, this is not common for legal texts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out the text\n",
    "Assuming we don't want to change the filters we can now use it to filter out the texts that we want to keep:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# 4. Filter out the documents that do not pass the quality\n",
    "legal_docs_filtered = [doc for doc in legal_docs if doc._.passed_quality_check]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print(\n",
    "    f\"We had a total of {len(legal['text'])} which we filtered down to {len(legal_docs_filtered)}.\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems like a lot, we should probably check why that is. We can do this by looking at the distribution of the scores of e.g. duplicate 10-gram fraction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def get_duplicate_10_gram_fraction(doc):\n",
    "    quality = doc._.quality\n",
    "    duplicate_10_gram_fraction = quality.duplicate_ngram_chr_fraction[\"10\"]\n",
    "    return duplicate_10_gram_fraction.value\n",
    "\n",
    "\n",
    "duplicate_10_gram_fraction = [get_duplicate_10_gram_fraction(doc) for doc in legal_docs]\n",
    "sns.histplot(duplicate_10_gram_fraction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like it explains a lot of the texts which were filtered out, but does not explain everything. Let us take a look at the `alpha_ratio` (the proportion of words which contains at least one alphabetic character) as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "alpha_ratio = [doc._.quality.alpha_ratio.value for doc in legal_docs]\n",
    "sns.histplot(alpha_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that most of the text do not pass the `alpha_ratio` threshold of 0.7 or higher. This is not uncommon for legal documents as e.g. the paragraph sign `ยง` is not an alphabetic character. It might be relevant to change the threshold to 0.7 or lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing across domains\n",
    "We see that legal documents have quite a few perculiarities let us examine how the `alpha_ratio` behaves across different domains:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# first we apply the pipeline to the other domains\n",
    "news_docs = nlp.pipe(news[\"text\"])\n",
    "news_docs = list(news_docs)\n",
    "speech_docs = nlp.pipe(speech[\"text\"])\n",
    "speech_docs = list(speech_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# extract alpha ratio:\n",
    "news_alpha_ratio = [doc._.quality.alpha_ratio.value for doc in news_docs]\n",
    "speech_alpha_ratio = [doc._.quality.alpha_ratio.value for doc in speech_docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the metrics we can plot a histogram comparing the metrics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# histogram\n",
    "sns.histplot(news_alpha_ratio, label=\"News\", alpha=0.5, binwidth=0.05)\n",
    "sns.histplot(alpha_ratio, label=\"Legal\", alpha=0.5, binwidth=0.05)\n",
    "sns.histplot(speech_alpha_ratio, label=\"Speech\", alpha=0.5, binwidth=0.05)\n",
    "\n",
    "# add labels\n",
    "plt.xlabel(\"Alpha ratio\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a couple of things:\n",
    "- A fair amount of legal documents have an alpha ratio above 0.6.\n",
    "- Almost no news text have a alpha ratio below 0.6.\n",
    "- The alpha ratio for the Speech corpus is suspicously low\n",
    "\n",
    "Let us examine one of the speech samples a bit more in-depth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "doc = speech_docs[0]\n",
    "# examine the first 100 tokens in the first document\n",
    "print(doc[:100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that a high proportion of the tokens in the speech dataset dentoes the speaker such and tokens such as `:` then lower the alpa ratio. This might or might not be problematic for the task at hand.\n",
    "\n",
    "**Therefore it is important to note that while these filters are useful for filtering large amount of texts it is also important to know that they should be adjusted to the target domain.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textdescriptives",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31387647799921bb85032eec7bb02e281325ae7f8ffa6f9cd7cdead815b36c88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
